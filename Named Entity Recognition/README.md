Welcome!

Named entity recognition is a task that classifies each word as one of several named entities. Modern NLP world is dominated by Transformer based models which are a specific type of neural network architecture. I used Huggingface's Transformers library that offers pre-trained implementations of state of the art models. Besides showing my ability to fine-tune a model for NER task I wanted to test if DeBERTAv3 xSmall with a backbone of only 22M parameters can outperform RoBERTa base which has 86M backone parameters. I've chosen these specific models because in DeBERTav3 paper autors claim that xSmall variant of their model can outperform RoBERTa base variant on some tasks. I tracked my experiments with Weights&Biases which allowed me to do a comprehensive comparison of metrics they achieved and also their performance during training. Thanks to Huggingface, inference API is avaliable here https://huggingface.co/Gozdi/roberta-base-finetuned-ner
