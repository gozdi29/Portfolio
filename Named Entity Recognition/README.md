Welcome!

In this project I fine-tuned two Transformers-based models to do Named Entity Recognition. NER is a task that classifies each word as one of several named entities.
I used Huggingface's Transformers library which offers easy to use implementations of modern Transformers-based models.

I had 2 goals doing this project. First one was to show my ability of using Transformers library for finetuning pre-trained models on NER task. 
Second one was to compare DeBERTAv3 xSmall results to RoBERTa Base results. In DeBERTAv3 authors say that their xSmall model, which has only 22M backbone parameters, can outperform
RoBERTA base, which has 86M backbone parameters, on some taska. I wanted to see if DeBERTav3 can also outperform RoBERTa on a dataset I chose.
