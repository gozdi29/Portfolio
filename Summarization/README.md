Welcome!

In this project I fine-tune one of T5-efficient variants on Samsum dataset to do abstractive summarization. Samsum is a dataset composed of short messenger like conversations written by linguists fluent in English. T5-efficient is a collections of augmented T5 models that were developed during a study of efficient scaling of transformers. I picked a variant that is small but has good metrics and outperforms other variants with comparable number of parameters. I tracked my experiments with Weights&Biases. Summaries generated by model fine-tuned by me sometimes are far from perfect but training a perfect model wasn't my goal. My goal was to show my ability and necessary knowledge for fine-tuning Transformer Encoder-Decoder model for text generation task such as summarization or translation.

Inference API is avaliable here https://huggingface.co/Gozdi/t5-efficient-small-nl16-samsum-exp2.

Here's also an example conversation:

Oscar: A coffee at Tristano's?

Payne: Why not. in 15 mins?

Oscar: let's make it half an hour ok?

Payne: great, i'll be there

Oscar: see you there
